{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff8700e-2423-4366-8e61-7cd4738f4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet llama-index-llms-mistralai llama-index-llms-groq llama-index-llms-anthropic llama-index-llms-gemini llama-index-llms-openai llama-index-llms-openai-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc911b0b-4e15-401e-9875-e43c25470daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Insert the parent directory of \"app\" into sys.path\n",
    "# so that Python recognizes \"app\" as an importable package.\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85258025-55e3-45db-b6a5-29004264c7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This looks for .env in the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70278d01-fe4a-47fe-91e5-ff91b67a046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomazbratanic/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/tomazbratanic/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/tomazbratanic/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.llms.groq import Groq\n",
    "from prettytable import PrettyTable\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "\n",
    "from workflows.shared import graph_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4329aafd-3e8c-4acb-abdf-fcedd3f95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import flows\n",
    "from workflows.naive_text2cypher import NaiveText2CypherFlow\n",
    "from workflows.naive_text2cypher_retry import NaiveText2CypherRetryFlow\n",
    "from workflows.text2cypher_retry_check import NaiveText2CypherRetryCheckFlow\n",
    "from workflows.iterative_planner import IterativePlanningFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041de039-c410-417f-bec2-9b92a70cdf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Cypher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who acted in Tom Hanks’s highest-rated movie?</td>\n",
       "      <td>MATCH (p:Person {name: 'Tom Hanks'})-[:ACTED_I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which movie starring Keanu Reeves has the most...</td>\n",
       "      <td>MATCH (meg:Actor {name: \"Keanu Reeves\"})-[:ACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who directed the most recent movie starring Ha...</td>\n",
       "      <td>MATCH (p:Person {name: \"Halle Berry\"})-[:ACTED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the highest-rated movie from the 1990s...</td>\n",
       "      <td>MATCH (m:Movie)-[:DIRECTED]-(d:Person) WHERE m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For all movies starring Keanu Reeves, find the...</td>\n",
       "      <td>MATCH (keanu:Person {name: \"Keanu Reeves\"})-[:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0      Who acted in Tom Hanks’s highest-rated movie?   \n",
       "1  Which movie starring Keanu Reeves has the most...   \n",
       "2  Who directed the most recent movie starring Ha...   \n",
       "3  What is the highest-rated movie from the 1990s...   \n",
       "4  For all movies starring Keanu Reeves, find the...   \n",
       "\n",
       "                                              Cypher  \n",
       "0  MATCH (p:Person {name: 'Tom Hanks'})-[:ACTED_I...  \n",
       "1  MATCH (meg:Actor {name: \"Keanu Reeves\"})-[:ACT...  \n",
       "2  MATCH (p:Person {name: \"Halle Berry\"})-[:ACTED...  \n",
       "3  MATCH (m:Movie)-[:DIRECTED]-(d:Person) WHERE m...  \n",
       "4  MATCH (keanu:Person {name: \"Keanu Reeves\"})-[:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark data\n",
    "test_df = pd.read_csv('test_data.csv', delimiter=\";\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a57e472-52c6-4da0-b6ef-d3122d5c4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_flow_llm_combination(flow_name, flow, llm_name, llm, test_df, graph_store):\n",
    "    results = []\n",
    "    latencies = []\n",
    "    ground_truth = []\n",
    "    timeouts = 0\n",
    "    flow_instance = flow(llm=llm, timeout=90) # Timeout 2 minutes\n",
    "\n",
    "    # Add tqdm progress bar\n",
    "    for i, row in tqdm(test_df.iterrows(), total=len(test_df),\n",
    "                      desc=f\"Evaluating {flow_name} with {llm_name}\"):\n",
    "        question = row['Question']\n",
    "\n",
    "        start = time.time()\n",
    "        try:\n",
    "            data = await flow_instance.run(input=question)\n",
    "        except:\n",
    "            data = {\"answer\": \"timeout/error\", \"question\": question}\n",
    "            timeouts += 1\n",
    "        end = time.time()\n",
    "        latencies.append(end - start)\n",
    "        results.append(data)\n",
    "\n",
    "        try:\n",
    "            ground_truth.append(str(graph_store.structured_query(row['Cypher'])))\n",
    "        except Exception as e:\n",
    "            ground_truth.append(\"missing\")\n",
    "\n",
    "    # Create evaluation dataset\n",
    "    df = pd.DataFrame(results)\n",
    "    df['ground_truth'] = ground_truth\n",
    "    df['latencies'] = latencies\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[answer_relevancy],\n",
    "        llm=LlamaIndexLLMWrapper(OpenAI(model=\"gpt-4o-2024-11-20\", temperature=0))\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'answer_relevancy': result['answer_relevancy'],\n",
    "        'avg_latency': sum(latencies) / len(latencies),\n",
    "        'timeout/errors': timeouts\n",
    "    }\n",
    "\n",
    "async def run_grid_search(\n",
    "    flows: List[callable],\n",
    "    llms: List[object],\n",
    "    test_df: pd.DataFrame,\n",
    "    graph_store: object\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for flow in flows:\n",
    "        for llm_name, llm in llms:\n",
    "            try:\n",
    "                print(f\"\\nEvaluating {flow.__name__} with {llm_name}\")\n",
    "\n",
    "                # Skip iterative planning only with these\n",
    "                if flow.__name__ == \"IterativePlanningFlow\" and not llm_name in [\"gpt-4o\", \"1.5pro\"]:\n",
    "                    continue\n",
    "\n",
    "                result = await evaluate_flow_llm_combination(\n",
    "                    flow_name=flow.__name__,\n",
    "                    flow=flow,\n",
    "                    llm_name=llm_name,\n",
    "                    llm=llm,\n",
    "                    test_df=test_df,\n",
    "                    graph_store=graph_store\n",
    "                )\n",
    "                print(result)\n",
    "                results.append({\n",
    "                    'flow': flow.__name__,\n",
    "                    'llm': llm_name,\n",
    "                    **result\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a7173-eb21-4b46-b78e-c6dbfcab1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = [\n",
    "    IterativePlanningFlow,\n",
    "    NaiveText2CypherFlow,\n",
    "    NaiveText2CypherRetryFlow,\n",
    "    NaiveText2CypherRetryCheckFlow\n",
    "]  # Add your flows\n",
    "\n",
    "google_retry = dict(retry=retry.Retry(initial=0.1, multiplier=2, timeout=61))\n",
    "llms = [\n",
    "    (\"1.5pro\", Gemini(model=\"models/gemini-1.5-pro\", temperature=0, request_options=google_retry)),\n",
    "    (\"1.5flash\", Gemini(model=\"models/gemini-1.5-flash\", temperature=0, request_options=google_retry)),\n",
    "    #(\"2.0flash\", Gemini(model=\"models/gemini-2.0-flash-exp\", temperature=0)), # rate limits\n",
    "    (\"gpt-4o\", OpenAI(model=\"gpt-4o\", temperature=0)),\n",
    "    #(\"gpt-4o-mini\", OpenAI(model=\"gpt-4o-mini\", temperature=0)),\n",
    "    (\"o1\", OpenAI(model=\"o1-preview\", temperature=0)), #no tools\n",
    "    (\"o1-mini\", OpenAI(model=\"o1-mini\", temperature=0)), #no tools\n",
    "    (\"sonnet 3.5\", Anthropic(model=\"claude-3-5-sonnet-latest\", max_tokens=8076)),\n",
    "    (\"haiku 3.5\", Anthropic(model=\"claude-3-5-haiku-latest\", max_tokens=8076)),\n",
    "    (\"mistral medium\", MistralAI(model=\"mistral-medium\")),\n",
    "    (\"mistral large\", MistralAI(model=\"mistral-large-latest\")),\n",
    "    (\"ministral 8b\", MistralAI(model=\"ministral-8b-latest\")),\n",
    "    (\"codestral\", MistralAI(model=\"codestral-latest\")),\n",
    "    (\"deepsek-v3\", OpenAILike(\n",
    "                            model=\"deepseek-chat\",\n",
    "                            api_base=\"https://api.deepseek.com/beta\",\n",
    "                            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "                        )\n",
    "                    ),\n",
    "    #(\"groq llama3 70b\", Groq(model=\"llama3-70b-8192\")) # Rate limits\n",
    "\n",
    "\n",
    "\n",
    "]  # Add your LLMs\n",
    "\n",
    "results = await run_grid_search(\n",
    "    flows=flows, llms=llms, test_df=test_df, graph_store=graph_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4961dd5-e34c-4698-8e10-b85f9ae7a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Results:\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n",
      "|              Flow              |      LLM       | Answer Relevancy | Timeouts/Errors | Avg Latency (s) |\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n",
      "| NaiveText2CypherRetryCheckFlow |   sonnet 3.5   |      0.843       |        0        |      25.45      |\n",
      "| NaiveText2CypherRetryCheckFlow |   deepsek-v3   |      0.837       |        0        |      29.11      |\n",
      "| NaiveText2CypherRetryCheckFlow |     gpt-4o     |      0.820       |        0        |      19.83      |\n",
      "| NaiveText2CypherRetryCheckFlow |    o1-mini     |      0.816       |        2        |      36.52      |\n",
      "| NaiveText2CypherRetryCheckFlow | mistral large  |      0.789       |        2        |      26.79      |\n",
      "| NaiveText2CypherRetryCheckFlow |       o1       |      0.746       |        12       |      57.09      |\n",
      "|   NaiveText2CypherRetryFlow    |   deepsek-v3   |      0.732       |        0        |       8.74      |\n",
      "|      NaiveText2CypherFlow      |   deepsek-v3   |      0.729       |        0        |       9.38      |\n",
      "| NaiveText2CypherRetryCheckFlow |   codestral    |      0.725       |        0        |      12.91      |\n",
      "|      NaiveText2CypherFlow      |     1.5pro     |      0.706       |        0        |       5.56      |\n",
      "| NaiveText2CypherRetryCheckFlow |     1.5pro     |      0.703       |        0        |      24.42      |\n",
      "|      NaiveText2CypherFlow      | mistral large  |      0.680       |        0        |       7.50      |\n",
      "|   NaiveText2CypherRetryFlow    |    o1-mini     |      0.674       |        0        |      18.09      |\n",
      "|   NaiveText2CypherRetryFlow    |     1.5pro     |      0.669       |        0        |       5.85      |\n",
      "| NaiveText2CypherRetryCheckFlow |   haiku 3.5    |      0.669       |        0        |      28.20      |\n",
      "|      NaiveText2CypherFlow      |    o1-mini     |      0.648       |        0        |      18.62      |\n",
      "|   NaiveText2CypherRetryFlow    | mistral large  |      0.625       |        0        |       8.97      |\n",
      "|      NaiveText2CypherFlow      |     gpt-4o     |      0.622       |        0        |       6.40      |\n",
      "|   NaiveText2CypherRetryFlow    |       o1       |      0.621       |        2        |      37.83      |\n",
      "|      NaiveText2CypherFlow      |   codestral    |      0.620       |        0        |       4.14      |\n",
      "|   NaiveText2CypherRetryFlow    |   sonnet 3.5   |      0.616       |        0        |      10.01      |\n",
      "|   NaiveText2CypherRetryFlow    |     gpt-4o     |      0.603       |        0        |       6.32      |\n",
      "|      NaiveText2CypherFlow      |   sonnet 3.5   |      0.596       |        0        |      10.55      |\n",
      "|   NaiveText2CypherRetryFlow    |   haiku 3.5    |      0.584       |        0        |       8.57      |\n",
      "|      NaiveText2CypherFlow      |       o1       |      0.567       |        6        |      43.13      |\n",
      "| NaiveText2CypherRetryCheckFlow |    1.5flash    |      0.558       |        0        |       9.48      |\n",
      "|   NaiveText2CypherRetryFlow    |   codestral    |      0.548       |        0        |       4.47      |\n",
      "|      NaiveText2CypherFlow      |   haiku 3.5    |      0.530       |        0        |       8.43      |\n",
      "| NaiveText2CypherRetryCheckFlow |  ministral 8b  |      0.493       |        0        |      16.20      |\n",
      "|   NaiveText2CypherRetryFlow    |  ministral 8b  |      0.485       |        0        |       6.33      |\n",
      "|   NaiveText2CypherRetryFlow    | mistral medium |      0.474       |        0        |      10.79      |\n",
      "| NaiveText2CypherRetryCheckFlow | mistral medium |      0.453       |        0        |      31.52      |\n",
      "|      NaiveText2CypherFlow      |    1.5flash    |      0.445       |        0        |       2.59      |\n",
      "|      NaiveText2CypherFlow      | mistral medium |      0.435       |        0        |       9.12      |\n",
      "|   NaiveText2CypherRetryFlow    |    1.5flash    |      0.411       |        0        |       2.77      |\n",
      "|      NaiveText2CypherFlow      |  ministral 8b  |      0.396       |        0        |       4.85      |\n",
      "|     IterativePlanningFlow      |     1.5pro     |      0.303       |        1        |      20.97      |\n",
      "|     IterativePlanningFlow      |     gpt-4o     |      0.163       |        0        |      24.68      |\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def print_results(results: List[Dict]):\n",
    "    # Create table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Flow\", \"LLM\", \"Answer Relevancy\", \"Timeouts/Errors\", \"Avg Latency (s)\"]\n",
    "\n",
    "    # Sort results by answer relevancy\n",
    "    sorted_results = sorted(results, key=lambda x: sum(x['answer_relevancy']) / len(x['answer_relevancy']), reverse=True)\n",
    "\n",
    "    # Add rows\n",
    "    for result in sorted_results:\n",
    "        answer_relevancy = sum(result['answer_relevancy']) / len(result['answer_relevancy'])\n",
    "        timeout_errors = result['timeout/errors']\n",
    "\n",
    "        table.add_row([\n",
    "            result['flow'],\n",
    "            result['llm'],\n",
    "            f\"{answer_relevancy:.3f}\" if isinstance(answer_relevancy, (float, int)) else str(answer_relevancy),\n",
    "            f\"{timeout_errors}\",\n",
    "            f\"{result['avg_latency']:.2f}\"\n",
    "        ])\n",
    "\n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(table)\n",
    "\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460e0c15-d3ac-4639-838b-6ff19a0358d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Results:\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n",
      "|              Flow              |      LLM       | Answer Relevancy | Timeouts/Errors | Avg Latency (s) |\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n",
      "|   NaiveText2CypherRetryFlow    |   deepsek-v3   |      0.732       |        0        |       8.74      |\n",
      "|   NaiveText2CypherRetryFlow    |    o1-mini     |      0.674       |        0        |      18.09      |\n",
      "|   NaiveText2CypherRetryFlow    |     1.5pro     |      0.669       |        0        |       5.85      |\n",
      "|   NaiveText2CypherRetryFlow    | mistral large  |      0.625       |        0        |       8.97      |\n",
      "|   NaiveText2CypherRetryFlow    |       o1       |      0.621       |        2        |      37.83      |\n",
      "|   NaiveText2CypherRetryFlow    |   sonnet 3.5   |      0.616       |        0        |      10.01      |\n",
      "|   NaiveText2CypherRetryFlow    |     gpt-4o     |      0.603       |        0        |       6.32      |\n",
      "|   NaiveText2CypherRetryFlow    |   haiku 3.5    |      0.584       |        0        |       8.57      |\n",
      "|   NaiveText2CypherRetryFlow    |   codestral    |      0.548       |        0        |       4.47      |\n",
      "|   NaiveText2CypherRetryFlow    |  ministral 8b  |      0.485       |        0        |       6.33      |\n",
      "|   NaiveText2CypherRetryFlow    | mistral medium |      0.474       |        0        |      10.79      |\n",
      "|   NaiveText2CypherRetryFlow    |    1.5flash    |      0.411       |        0        |       2.77      |\n",
      "| NaiveText2CypherRetryCheckFlow |   sonnet 3.5   |      0.843       |        0        |      25.45      |\n",
      "| NaiveText2CypherRetryCheckFlow |   deepsek-v3   |      0.837       |        0        |      29.11      |\n",
      "| NaiveText2CypherRetryCheckFlow |     gpt-4o     |      0.820       |        0        |      19.83      |\n",
      "| NaiveText2CypherRetryCheckFlow |    o1-mini     |      0.816       |        2        |      36.52      |\n",
      "| NaiveText2CypherRetryCheckFlow | mistral large  |      0.789       |        2        |      26.79      |\n",
      "| NaiveText2CypherRetryCheckFlow |       o1       |      0.746       |        12       |      57.09      |\n",
      "| NaiveText2CypherRetryCheckFlow |   codestral    |      0.725       |        0        |      12.91      |\n",
      "| NaiveText2CypherRetryCheckFlow |     1.5pro     |      0.703       |        0        |      24.42      |\n",
      "| NaiveText2CypherRetryCheckFlow |   haiku 3.5    |      0.669       |        0        |      28.20      |\n",
      "| NaiveText2CypherRetryCheckFlow |    1.5flash    |      0.558       |        0        |       9.48      |\n",
      "| NaiveText2CypherRetryCheckFlow |  ministral 8b  |      0.493       |        0        |      16.20      |\n",
      "| NaiveText2CypherRetryCheckFlow | mistral medium |      0.453       |        0        |      31.52      |\n",
      "|      NaiveText2CypherFlow      |   deepsek-v3   |      0.729       |        0        |       9.38      |\n",
      "|      NaiveText2CypherFlow      |     1.5pro     |      0.706       |        0        |       5.56      |\n",
      "|      NaiveText2CypherFlow      | mistral large  |      0.680       |        0        |       7.50      |\n",
      "|      NaiveText2CypherFlow      |    o1-mini     |      0.648       |        0        |      18.62      |\n",
      "|      NaiveText2CypherFlow      |     gpt-4o     |      0.622       |        0        |       6.40      |\n",
      "|      NaiveText2CypherFlow      |   codestral    |      0.620       |        0        |       4.14      |\n",
      "|      NaiveText2CypherFlow      |   sonnet 3.5   |      0.596       |        0        |      10.55      |\n",
      "|      NaiveText2CypherFlow      |       o1       |      0.567       |        6        |      43.13      |\n",
      "|      NaiveText2CypherFlow      |   haiku 3.5    |      0.530       |        0        |       8.43      |\n",
      "|      NaiveText2CypherFlow      |    1.5flash    |      0.445       |        0        |       2.59      |\n",
      "|      NaiveText2CypherFlow      | mistral medium |      0.435       |        0        |       9.12      |\n",
      "|      NaiveText2CypherFlow      |  ministral 8b  |      0.396       |        0        |       4.85      |\n",
      "|     IterativePlanningFlow      |     1.5pro     |      0.303       |        1        |      20.97      |\n",
      "|     IterativePlanningFlow      |     gpt-4o     |      0.163       |        0        |      24.68      |\n",
      "+--------------------------------+----------------+------------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def print_by_flow_results(results: List[Dict]):\n",
    "    # Create table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Flow\", \"LLM\", \"Answer Relevancy\", \"Timeouts/Errors\", \"Avg Latency (s)\"]\n",
    "\n",
    "    # Sort results first by flow, then by answer relevancy\n",
    "    sorted_results = sorted(results,\n",
    "        key=lambda x: (\n",
    "            x['flow'],\n",
    "            sum(x['answer_relevancy']) / len(x['answer_relevancy'])\n",
    "        ),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Add rows\n",
    "    for result in sorted_results:\n",
    "        answer_relevancy = sum(result['answer_relevancy']) / len(result['answer_relevancy'])\n",
    "        timeout_errors = result['timeout/errors']\n",
    "\n",
    "        table.add_row([\n",
    "            result['flow'],\n",
    "            result['llm'],\n",
    "            f\"{answer_relevancy:.3f}\" if isinstance(answer_relevancy, (float, int)) else str(answer_relevancy),\n",
    "            f\"{timeout_errors}\",\n",
    "            f\"{result['avg_latency']:.2f}\"\n",
    "        ])\n",
    "\n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(table)\n",
    "\n",
    "print_by_flow_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f48ba6-7981-4628-9e8c-174f7e145cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pivot_results(results: List[Dict]):\n",
    "    # Get unique flows and LLMs\n",
    "    flows = list(set(r['flow'] for r in results))\n",
    "    llms = sorted(list(set(r['llm'] for r in results)))\n",
    "\n",
    "    # Create table\n",
    "    table = PrettyTable()\n",
    "\n",
    "    # Set field names with flows as columns\n",
    "    table.field_names = [\"LLM\"] + flows\n",
    "\n",
    "    # Create a dictionary to store relevancy scores\n",
    "    relevancy_dict = {}\n",
    "    for result in results:\n",
    "        llm = result['llm']\n",
    "        flow = result['flow']\n",
    "        relevancy = sum(result['answer_relevancy']) / len(result['answer_relevancy'])\n",
    "        if llm not in relevancy_dict:\n",
    "            relevancy_dict[llm] = {}\n",
    "        relevancy_dict[llm][flow] = f\"{relevancy:.3f}\"\n",
    "\n",
    "    # Add rows for each LLM\n",
    "    for llm in llms:\n",
    "        row = [llm]\n",
    "        for flow in flows:\n",
    "            row.append(relevancy_dict[llm].get(flow, \"N/A\"))\n",
    "        table.add_row(row)\n",
    "\n",
    "    print(\"\\nGrid Search Results (Answer Relevancy):\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6793799-06f9-4602-86d1-a6ab2bf6d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Results (Answer Relevancy):\n",
      "+----------------+-----------------------+----------------------+---------------------------+--------------------------------+\n",
      "|      LLM       | IterativePlanningFlow | NaiveText2CypherFlow | NaiveText2CypherRetryFlow | NaiveText2CypherRetryCheckFlow |\n",
      "+----------------+-----------------------+----------------------+---------------------------+--------------------------------+\n",
      "|    1.5flash    |          N/A          |        0.445         |           0.411           |             0.558              |\n",
      "|     1.5pro     |         0.303         |        0.706         |           0.669           |             0.703              |\n",
      "|   codestral    |          N/A          |        0.620         |           0.548           |             0.725              |\n",
      "|   deepsek-v3   |          N/A          |        0.729         |           0.732           |             0.837              |\n",
      "|     gpt-4o     |         0.163         |        0.622         |           0.603           |             0.820              |\n",
      "|   haiku 3.5    |          N/A          |        0.530         |           0.584           |             0.669              |\n",
      "|  ministral 8b  |          N/A          |        0.396         |           0.485           |             0.493              |\n",
      "| mistral large  |          N/A          |        0.680         |           0.625           |             0.789              |\n",
      "| mistral medium |          N/A          |        0.435         |           0.474           |             0.453              |\n",
      "|       o1       |          N/A          |        0.567         |           0.621           |             0.746              |\n",
      "|    o1-mini     |          N/A          |        0.648         |           0.674           |             0.816              |\n",
      "|   sonnet 3.5   |          N/A          |        0.596         |           0.616           |             0.843              |\n",
      "+----------------+-----------------------+----------------------+---------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_pivot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fdd12-0501-488a-b6a0-44ad95d46093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
